<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>imodelsx.llm API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodelsx.llm</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodelsx.llm.get_llm"><code class="name flex">
<span>def <span class="ident">get_llm</span></span>(<span>checkpoint,<br>seed=1,<br>role: str = None,<br>repeat_delay: float | None = None,<br>CACHE_DIR='/home/chansingh/.CACHE_LLM')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_llm(
    checkpoint,
    seed=1,
    role: str = None,
    repeat_delay: Optional[float] = None,
    CACHE_DIR=LLM_CONFIG[&#34;CACHE_DIR&#34;],
):
    if repeat_delay is not None:
        LLM_CONFIG[&#34;LLM_REPEAT_DELAY&#34;] = repeat_delay

    &#34;&#34;&#34;Get an LLM with a call function and caching capabilities&#34;&#34;&#34;
    if any(checkpoint.startswith(prefix) for prefix in [&#34;gpt-3&#34;, &#34;gpt-4&#34;, &#34;o3&#34;, &#34;o4&#34;, &#34;gpt-5&#34;]):
        return LLM_Chat(checkpoint, seed, role, CACHE_DIR)
    elif &#39;meta-llama&#39; in checkpoint and &#39;Instruct&#39; in checkpoint:
        if os.environ[&#39;HF_TOKEN&#39;] is None:
            raise ValueError(
                &#34;You must set the HF_TOKEN environment variable to use this model.&#34;)
        return LLM_HF_Pipeline(checkpoint, CACHE_DIR)
    else:
        # warning: this sets torch.manual_seed(seed)
        return LLM_HF(checkpoint, seed=seed, CACHE_DIR=CACHE_DIR)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm.load_hf_model"><code class="name flex">
<span>def <span class="ident">load_hf_model</span></span>(<span>checkpoint: str) ‑> transformers.modeling_utils.PreTrainedModel</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_hf_model(checkpoint: str) -&gt; transformers.PreTrainedModel:
    # set checkpoint
    kwargs = {
        &#34;pretrained_model_name_or_path&#34;: checkpoint,
        &#34;output_hidden_states&#34;: False,
        # &#34;pad_token_id&#34;: tokenizer.eos_token_id,
        &#34;low_cpu_mem_usage&#34;: True,
    }
    if &#34;google/flan&#34; in checkpoint:
        return T5ForConditionalGeneration.from_pretrained(
            checkpoint, device_map=&#34;auto&#34;, torch_dtype=torch.float16
        )
    elif checkpoint == &#34;EleutherAI/gpt-j-6B&#34;:
        return AutoModelForCausalLM.from_pretrained(
            checkpoint,
            revision=&#34;float16&#34;,
            torch_dtype=torch.float16,
            **kwargs,
        )
    elif &#34;llama-2&#34; in checkpoint.lower():
        return AutoModelForCausalLM.from_pretrained(
            checkpoint,
            torch_dtype=torch.float16,
            device_map=&#34;auto&#34;,
            token=os.environ[&#39;HF_TOKEN&#39;],
            offload_folder=&#34;offload&#34;,
        )
    elif &#39;microsoft/phi&#39; in checkpoint:
        return AutoModelForCausalLM.from_pretrained(
            checkpoint
        )
    elif checkpoint == &#34;gpt-xl&#34;:
        return AutoModelForCausalLM.from_pretrained(checkpoint)
    else:
        return AutoModelForCausalLM.from_pretrained(
            checkpoint,
            device_map=&#34;auto&#34;,
            torch_dtype=torch.float16,
            token=os.environ[&#39;HF_TOKEN&#39;],
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm.load_tokenizer"><code class="name flex">
<span>def <span class="ident">load_tokenizer</span></span>(<span>checkpoint: str) ‑> transformers.tokenization_utils.PreTrainedTokenizer</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_tokenizer(checkpoint: str) -&gt; transformers.PreTrainedTokenizer:
    if &#34;facebook/opt&#34; in checkpoint:
        # opt can&#39;t use fast tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            checkpoint, use_fast=False, padding_side=&#39;left&#39;, token=os.environ[&#39;HF_TOKEN&#39;])
    elif &#34;PMC_LLAMA&#34; in checkpoint:
        tokenizer = transformers.LlamaTokenizer.from_pretrained(
            &#34;chaoyi-wu/PMC_LLAMA_7B&#34;, padding_side=&#39;left&#39;, token=os.environ[&#39;HF_TOKEN&#39;])
    else:
        # , use_fast=True)
        tokenizer = AutoTokenizer.from_pretrained(
            checkpoint, padding_side=&#39;left&#39;, use_fast=True, token=os.environ[&#39;HF_TOKEN&#39;])

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    return tokenizer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm.repeatedly_call_with_delay"><code class="name flex">
<span>def <span class="ident">repeatedly_call_with_delay</span></span>(<span>llm_call)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def repeatedly_call_with_delay(llm_call):
    def wrapper(*args, **kwargs):
        # Number of seconds to wait between calls (None will not repeat)
        delay = LLM_CONFIG[&#34;LLM_REPEAT_DELAY&#34;]
        response = None
        while response is None:
            try:
                response = llm_call(*args, **kwargs)

                # fix for when this function was returning response rather than string
                # if response is not None and not isinstance(response, str):
                # response = response[&#34;choices&#34;][0][&#34;message&#34;][&#34;content&#34;]
            except Exception as e:
                e = str(e)
                print(e)
                if &#34;does not exist&#34; in e:
                    return None
                elif &#34;maximum context length&#34; in e:
                    return None
                elif &#39;content management policy&#39; in e:
                    return None
                if delay is None:
                    raise e
                else:
                    time.sleep(delay)
        return response

    return wrapper</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodelsx.llm.LLMEmbs"><code class="flex name class">
<span>class <span class="ident">LLMEmbs</span></span>
<span>(</span><span>checkpoint)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLMEmbs:
    def __init__(self, checkpoint):
        self.tokenizer_ = load_tokenizer(checkpoint)
        self.model_ = AutoModel.from_pretrained(
            checkpoint, output_hidden_states=True,
            device_map=&#34;auto&#34;,
            torch_dtype=torch.float16,)

    def __call__(self, texts: List[str], layer_idx: int = 18, batch_size=16):
        &#39;&#39;&#39;Returns embeddings
        &#39;&#39;&#39;
        embs = []
        for i in tqdm(range(0, len(texts), batch_size)):
            inputs = self.tokenizer_(
                texts[i:i + batch_size], return_tensors=&#39;pt&#39;, padding=True).to(self.model_.device)
            hidden_states = self.model_(**inputs).hidden_states

            # layers x batch x tokens x features
            emb = hidden_states[layer_idx].detach().cpu().numpy()

            # get emb from last token
            emb = emb[:, -1, :]
            embs.append(deepcopy(emb))
        embs = np.concatenate(embs)
        return embs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm.LLM_Chat"><code class="flex name class">
<span>class <span class="ident">LLM_Chat</span></span>
<span>(</span><span>checkpoint, seed=1, role=None, CACHE_DIR='/home/chansingh/.CACHE_LLM')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLM_Chat:
    &#34;&#34;&#34;Chat models take a different format: https://platform.openai.com/docs/guides/chat/introduction&#34;&#34;&#34;

    def __init__(self, checkpoint, seed=1, role=None, CACHE_DIR=LLM_CONFIG[&#34;CACHE_DIR&#34;]):
        self.cache_dir = join(
            CACHE_DIR, &#34;cache_openai&#34;, f&#39;{checkpoint.replace(&#34;/&#34;, &#34;_&#34;)}___{seed}&#39;
        )
        self.checkpoint = checkpoint
        self.role = role
        from openai import AzureOpenAI
        from azure.identity import ChainedTokenCredential, AzureCliCredential, ManagedIdentityCredential, get_bearer_token_provider

        try:
            client_id = os.environ.get(&#34;AZURE_CLIENT_ID&#34;)
            scope = &#34;https://cognitiveservices.azure.com/.default&#34;
            credential = get_bearer_token_provider(ChainedTokenCredential(
                AzureCliCredential(), # first check local
                ManagedIdentityCredential(client_id=client_id)
            ), scope)
            if &#39;audio&#39; in checkpoint:
                self.client = AzureOpenAI(
                    api_version=&#34;2025-01-01-preview&#34;,
                    azure_endpoint=&#34;https://neuroaiservice.cognitiveservices.azure.com/openai/deployments/gpt-4o-audio-preview/chat/completions?api-version=2025-01-01-preview&#34;,
                    azure_ad_token_provider=credential,
                    timeout=10,
                    max_retries=3,
                )
            elif &#39;gpt-5&#39; in checkpoint:
                self.client = AzureOpenAI(
                    api_version=&#34;2025-01-01-preview&#34;,
                    azure_endpoint=&#34;https://dl-openai-3.openai.azure.com/&#34;,
                    azure_ad_token_provider=credential
                )
            else:
                self.client = AzureOpenAI(
                    api_version=&#34;2025-01-01-preview&#34;,
                    azure_endpoint=&#34;https://dl-openai-1.openai.azure.com/&#34;,
                    azure_ad_token_provider=credential
                )
        except Exception as e:
            print(&#39;failed to create client&#39;, e)
            print(&#39;You may need to edit this call in order to supply your own OpenAI / AzureOpenAI key and authentication.&#39;)
            traceback.print_exc()

    @repeatedly_call_with_delay
    def __call__(
        self,
        prompts_list: List[Dict[str, str]],
        max_completion_tokens=250,
        stop=None,
        functions: List[Dict] = None,
        return_str=True,
        verbose=True,
        temperature=0,
        frequency_penalty=0.25,
        use_cache=True,
        return_false_if_not_cached=False,
        reasoning_effort=&#39;high&#39;,
        seed=1,
    ):
        &#34;&#34;&#34;
        prompts_list: list of dicts, each dict has keys &#39;role&#39; and &#39;content&#39;
            Example: [
                {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who won the world series in 2020?&#34;},
                {&#34;role&#34;: &#34;assistant&#34;,
                    &#34;content&#34;: &#34;The Los Angeles Dodgers won the World Series in 2020.&#34;},
                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where was it played?&#34;}
            ]
        prompts_list: str
            Alternatively, string which gets formatted into basic prompts_list:
            messages = [
                {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &lt;&lt;&lt;&lt;&lt;prompts_list&gt;&gt;&gt;&gt;},
            ]
        &#34;&#34;&#34;
        if isinstance(prompts_list, str):
            role = self.role
            if role is None:
                role = &#34;You are a helpful assistant.&#34;
            prompts_list = [
                {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: role},
                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompts_list},
            ]

        assert isinstance(prompts_list, list), prompts_list
        # breakpoint()

        # cache
        os.makedirs(self.cache_dir, exist_ok=True)
        prompts_list_dict = {
            str(i): sorted(v.items()) for i, v in enumerate(prompts_list)
        }
        prompts_list_dict[&#34;checkpoint&#34;] = self.checkpoint
        prompts_list_dict[&#34;temperature&#34;] = temperature
        prompts_list_dict[&#34;functions&#34;] = functions
        prompts_list_dict[&#34;max_completion_tokens&#34;] = max_completion_tokens
        if not seed == 1:
            prompts_list_dict[&#34;seed&#34;] = seed
        if not reasoning_effort == &#39;high&#39;:
            prompts_list_dict[&#39;reasoning_effort&#39;] = reasoning_effort
        
        dict_as_str = json.dumps(prompts_list_dict, sort_keys=True)
        hash_str = hashlib.sha256(dict_as_str.encode()).hexdigest()
        cache_file = join(
            self.cache_dir,
            f&#34;chat__{hash_str}.pkl&#34;,
        )
        if os.path.exists(cache_file) and use_cache:
            if verbose:
                print(&#34;cached!&#34;)
                # print(cache_file)
            # print(cache_file)
            response = pkl.load(open(cache_file, &#34;rb&#34;))
            if response is not None:
                return response
        if verbose:
            print(&#34;not cached&#34;)

        if return_false_if_not_cached:
            return False

        kwargs = dict(
            model=self.checkpoint,
            messages=prompts_list,
            max_completion_tokens=max_completion_tokens,
            temperature=temperature,
            top_p=1,
            frequency_penalty=frequency_penalty,  # maximum is 2
            presence_penalty=0,
            stop=stop,
            reasoning_effort=reasoning_effort,
            # logprobs=True,
            # stop=[&#34;101&#34;]
        )
        # print(&#39;kwargs&#39;, kwargs)
        if functions is not None:
            kwargs[&#34;functions&#34;] = functions
        if self.checkpoint.startswith(&#39;o&#39;) or self.checkpoint == &#39;gpt-5&#39;:
            del kwargs[&#39;temperature&#39;]  # o3 and o4 don&#39;t support temperature
            del kwargs[&#39;frequency_penalty&#39;]
            del kwargs[&#39;top_p&#39;]
        
        if not &#39;gpt-5&#39; in self.checkpoint:
            del kwargs[&#39;reasoning_effort&#39;]


        response = self.client.chat.completions.create(
            **kwargs,
        )

        if return_str:
            response = response.choices[0].message.content

        if response is not None:
            # print(&#39;resp&#39;, response, &#39;cache_file&#39;, cache_file)
            try:
                # print(cache_file, &#39;cached!&#39;)
                pkl.dump(response, open(cache_file, &#34;wb&#34;))
            except:
                print(&#39;failed to save cache!&#39;, cache_file)
                traceback.print_exc()

        return response</code></pre>
</details>
<div class="desc"><p>Chat models take a different format: <a href="https://platform.openai.com/docs/guides/chat/introduction">https://platform.openai.com/docs/guides/chat/introduction</a></p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodelsx.llm.LLM_Chat_Audio" href="#imodelsx.llm.LLM_Chat_Audio">LLM_Chat_Audio</a></li>
</ul>
</dd>
<dt id="imodelsx.llm.LLM_Chat_Audio"><code class="flex name class">
<span>class <span class="ident">LLM_Chat_Audio</span></span>
<span>(</span><span>checkpoint, seed=1, role=None, CACHE_DIR='/home/chansingh/.CACHE_LLM')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLM_Chat_Audio(LLM_Chat):

    def __call__(
        self,
        prompt_str: str,
        audio_str: str,
        return_str=True,
        verbose=True,
        use_cache=True,
        return_false_if_not_cached=False,
    ):

        # breakpoint()
        prompts_list = [{
            &#34;role&#34;: &#34;user&#34;,
            &#34;content&#34;: [
                {
                    &#34;type&#34;: &#34;text&#34;,
                    &#34;text&#34;: prompt_str,
                    # &#34;text&#34;: &#39;What food is mentioned in the recording?&#39;,
                },
                {
                    &#34;type&#34;: &#34;input_audio&#34;,
                    &#34;input_audio&#34;: {
                        &#34;data&#34;: audio_str,
                        &#34;format&#34;: &#34;wav&#34;
                    }
                }
            ]
        }]

        # cache
        os.makedirs(self.cache_dir, exist_ok=True)
        prompts_list_dict = {
            str(i): sorted(v.items()) for i, v in enumerate(prompts_list)
        }
        dict_as_str = json.dumps(prompts_list_dict, sort_keys=True)
        hash_str = hashlib.sha256(dict_as_str.encode()).hexdigest()
        cache_file = join(
            self.cache_dir,
            f&#34;audio__{hash_str}.pkl&#34;,
        )
        if os.path.exists(cache_file) and use_cache:
            if verbose:
                print(&#34;cached!&#34;)
                # print(cache_file)
            # print(cache_file)
            response = pkl.load(open(cache_file, &#34;rb&#34;))
            if response is not None:
                return response
        if verbose:
            print(&#34;not cached&#34;)

        if return_false_if_not_cached:
            return False

        kwargs = dict(
            model=self.checkpoint,
            messages=prompts_list,
            # temperature=0,
        )
        response = self.client.chat.completions.create(
            modalities=[&#34;text&#34;, &#34;audio&#34;],
            audio={&#34;voice&#34;: &#34;alloy&#34;, &#34;format&#34;: &#34;wav&#34;},
            **kwargs,
        )

        if return_str:
            response = response.choices[0].message.audio.transcript

        if response is not None:
            # print(&#39;resp&#39;, response, &#39;cache_file&#39;, cache_file)
            try:
                # print(cache_file, &#39;cached!&#39;)
                pkl.dump(response, open(cache_file, &#34;wb&#34;))
            except:
                print(&#39;failed to save cache!&#39;, cache_file)
                traceback.print_exc()

        return response</code></pre>
</details>
<div class="desc"><p>Chat models take a different format: <a href="https://platform.openai.com/docs/guides/chat/introduction">https://platform.openai.com/docs/guides/chat/introduction</a></p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodelsx.llm.LLM_Chat" href="#imodelsx.llm.LLM_Chat">LLM_Chat</a></li>
</ul>
</dd>
<dt id="imodelsx.llm.LLM_HF"><code class="flex name class">
<span>class <span class="ident">LLM_HF</span></span>
<span>(</span><span>checkpoint, seed, CACHE_DIR)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLM_HF:
    def __init__(self, checkpoint, seed, CACHE_DIR):
        self.tokenizer_ = load_tokenizer(checkpoint)
        self.model_ = load_hf_model(checkpoint)
        self.checkpoint = checkpoint
        if CACHE_DIR is not None:
            self.cache_dir = join(
                CACHE_DIR, &#34;cache_hf&#34;, f&#39;{checkpoint.replace(&#34;/&#34;, &#34;_&#34;)}___{seed}&#39;
            )
        else:
            self.cache_dir = None
        self.seed = seed

    def __call__(
        self,
        prompt: Union[str, List[str]],
        stop: str = None,
        max_new_tokens=20,
        do_sample=False,
        use_cache=True,
        verbose=False,
        return_next_token_prob_scores=False,
        target_token_strs: List[str] = None,
        return_top_target_token_str: bool = False,
        batch_size=1,
    ) -&gt; Union[str, List[str]]:
        &#34;&#34;&#34;Warning: stop is used posthoc but not during generation.
        Be careful, caching can take up a lot of memory....

        Example mistral-instruct prompt: &#34;&lt;s&gt;[INST]&#39;Input text: {example}\nQuestion: {question} Answer yes or no.[/INST]&#34;


        Params
        ------
        return_next_token_prob_scores: bool
            If this is true, then the function will return the probability of the next token being each of the target_token_strs
            target_token_strs: List[str]
                If this is not None and return_next_token_prob_scores is True, then the function will return the probability of the next token being each of the target_token_strs
                The output will be a list of dictionaries in this case List[Dict[str, float]]
                return_top_target_token_str: bool
                    If true and above are true, then just return top token of the above
                    This is a way to constrain the output (but only for 1 token)
                    This setting caches but the other two (which do not return strings) do not cache

        &#34;&#34;&#34;
        input_is_str = isinstance(prompt, str)
        with torch.no_grad():
            use_cache = use_cache and self.cache_dir is not None
            # cache
            if use_cache:
                os.makedirs(self.cache_dir, exist_ok=True)
                hash_str = hashlib.sha256(str(prompt).encode()).hexdigest()
                cache_file = join(
                    self.cache_dir, f&#34;{hash_str}__num_tok={max_new_tokens}.pkl&#34;
                )

                if os.path.exists(cache_file):
                    if verbose:
                        print(&#34;cached!&#34;)
                    try:
                        return pkl.load(open(cache_file, &#34;rb&#34;))
                    except:
                        print(&#39;failed to load cache so rerunning...&#39;)
                if verbose:
                    print(&#34;not cached...&#34;)

            # if stop is not None:
            # raise ValueError(&#34;stop kwargs are not permitted.&#34;)
            inputs = self.tokenizer_(
                prompt, return_tensors=&#34;pt&#34;,
                return_attention_mask=True,
                padding=True,
                truncation=False,
            ).to(self.model_.device)

            if return_next_token_prob_scores or target_token_strs or return_top_target_token_str:
                outputs = self.model_.generate(
                    **inputs,
                    max_new_tokens=1,
                    pad_token_id=self.tokenizer_.pad_token_id,
                    output_logits=True,
                    return_dict_in_generate=True,
                )
                next_token_logits = outputs[&#39;logits&#39;][0]
                next_token_probs = next_token_logits.softmax(
                    axis=-1).detach().cpu().numpy()

                if target_token_strs is None:
                    return next_token_probs

                target_token_ids = self._check_target_token_strs(
                    target_token_strs)
                if return_top_target_token_str:
                    selected_tokens = next_token_probs[:, np.array(
                        target_token_ids)].squeeze().argmax(axis=-1)
                    out_strs = [
                        target_token_strs[selected_tokens[i]]
                        for i in range(len(selected_tokens))
                    ]
                    if len(out_strs) == 1:
                        out_strs = out_strs[0]
                    if use_cache:
                        pkl.dump(out_strs, open(cache_file, &#34;wb&#34;))
                    return out_strs
                else:
                    out_dict_list = [
                        {target_token_strs[i]: next_token_probs[prompt_num, target_token_ids[i]]
                            for i in range(len(target_token_strs))
                         }
                        for prompt_num in range(len(prompt))
                    ]
                    return out_dict_list
            else:
                outputs = self.model_.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer_.pad_token_id,
                )
                # top_p=0.92,
                # temperature=0,
                # top_k=0
            if input_is_str:
                out_str = self.tokenizer_.decode(
                    outputs[0], skip_special_tokens=True)
                # print(&#39;out_str&#39;, out_str)
                if &#39;mistral&#39; in self.checkpoint and &#39;Instruct&#39; in self.checkpoint:
                    out_str = out_str[len(prompt) - 2:]
                elif &#39;Meta-Llama-3&#39; in self.checkpoint and &#39;Instruct&#39; in self.checkpoint:
                    out_str = out_str[len(prompt) - 145:]
                else:
                    out_str = out_str[len(prompt):]

                if use_cache:
                    pkl.dump(out_str, open(cache_file, &#34;wb&#34;))
                return out_str
            else:
                out_strs = []
                for i in range(outputs.shape[0]):
                    out_tokens = outputs[i]
                    out_str = self.tokenizer_.decode(
                        out_tokens, skip_special_tokens=True)
                    if &#39;Ministral&#39; in self.checkpoint and &#39;Instruct&#39; in self.checkpoint:
                        out_str = out_str[len(prompt[i]) - 16:]
                    elif &#39;Qwen&#39; in self.checkpoint:
                        out_str = out_str[len(prompt[i]) - 34:]
                    elif &#39;mistral&#39; in self.checkpoint and &#39;Instruct&#39; in self.checkpoint:
                        out_str = out_str[len(prompt[i]) - 2:]
                    elif &#39;Meta-Llama-3&#39; in self.checkpoint and &#39;Instruct&#39; in self.checkpoint:
                        # print(&#39;here&#39;)
                        out_str = out_str[len(prompt) + 187:]
                    else:
                        out_str = out_str[len(prompt[i]):]
                    out_strs.append(out_str)
                if use_cache:
                    pkl.dump(out_strs, open(cache_file, &#34;wb&#34;))
                return out_strs

    def _check_target_token_strs(self, target_token_strs, override_token_with_first_token_id=False):
        if isinstance(target_token_strs, str):
            target_token_strs = [target_token_strs]

        target_token_ids = [self.tokenizer_(target_token_str, add_special_tokens=False)[&#34;input_ids&#34;]
                            for target_token_str in target_token_strs]

        # Check that the target token is in the vocab
        if override_token_with_first_token_id:
            # Get first token id in target_token_str
            target_token_ids = [target_token_id[0]
                                for target_token_id in target_token_ids]
        else:
            for i in range(len(target_token_strs)):
                if len(target_token_ids[i]) &gt; 1:
                    raise ValueError(
                        f&#34;target_token_str {target_token_strs[i]} has multiple tokens: &#34; +
                        str([self.tokenizer_.decode(target_token_id)
                            for target_token_id in target_token_ids[i]]))
        return target_token_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm.LLM_HF_Pipeline"><code class="flex name class">
<span>class <span class="ident">LLM_HF_Pipeline</span></span>
<span>(</span><span>checkpoint, CACHE_DIR)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLM_HF_Pipeline:
    def __init__(self, checkpoint, CACHE_DIR):

        self.pipeline_ = transformers.pipeline(
            &#34;text-generation&#34;,
            model=checkpoint,
            model_kwargs={&#34;torch_dtype&#34;: torch.bfloat16},
            # &#39;device_map&#39;: &#34;auto&#34;},
            # model_kwargs={&#39;torch_dtype&#39;: torch.float16},
            device_map=&#34;auto&#34;,
        )
        self.pipeline_.tokenizer.pad_token_id = self.pipeline_.tokenizer.eos_token_id
        self.pipeline_.tokenizer.padding_side = &#39;left&#39;
        # self.pipeline_.model.generation_config.pad_token_id = self.pipeline_.tokenizer.pad_token_id
        self.cache_dir = CACHE_DIR

    def __call__(
        self,
        prompt: Union[str, List[str]],
        max_new_tokens=20,
        use_cache=True,
        verbose=False,
        batch_size=64,
    ):
        use_cache = use_cache and self.cache_dir is not None
        if use_cache:
            os.makedirs(self.cache_dir, exist_ok=True)
            hash_str = hashlib.sha256(str(prompt).encode()).hexdigest()
            cache_file = join(
                self.cache_dir, f&#34;{hash_str}__num_tok={max_new_tokens}.pkl&#34;
            )

            if os.path.exists(cache_file):
                if verbose:
                    print(&#34;cached!&#34;)
                try:
                    return pkl.load(open(cache_file, &#34;rb&#34;))
                except:
                    print(&#39;failed to load cache so rerunning...&#39;)
            if verbose:
                print(&#34;not cached...&#34;)
        outputs = self.pipeline_(
            prompt,
            max_new_tokens=max_new_tokens,
            batch_size=batch_size,
            do_sample=False,
            pad_token_id=self.pipeline_.tokenizer.pad_token_id,
            top_p=None,
            temperature=None,
        )
        # print(outputs)
        if isinstance(prompt, str):
            texts = outputs[0][&#34;generated_text&#34;][len(prompt):]
        else:
            texts = [outputs[i][0][&#39;generated_text&#39;]
                     [len(prompt[i]):] for i in range(len(outputs))]

        if use_cache:
            pkl.dump(texts, open(cache_file, &#34;wb&#34;))
        return texts</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder"><code class="flex name class">
<span>class <span class="ident">LlamaTokenizer</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, *args, **kwargs):
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodelsx" href="index.html">imodelsx</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imodelsx.llm.get_llm" href="#imodelsx.llm.get_llm">get_llm</a></code></li>
<li><code><a title="imodelsx.llm.load_hf_model" href="#imodelsx.llm.load_hf_model">load_hf_model</a></code></li>
<li><code><a title="imodelsx.llm.load_tokenizer" href="#imodelsx.llm.load_tokenizer">load_tokenizer</a></code></li>
<li><code><a title="imodelsx.llm.repeatedly_call_with_delay" href="#imodelsx.llm.repeatedly_call_with_delay">repeatedly_call_with_delay</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodelsx.llm.LLMEmbs" href="#imodelsx.llm.LLMEmbs">LLMEmbs</a></code></h4>
</li>
<li>
<h4><code><a title="imodelsx.llm.LLM_Chat" href="#imodelsx.llm.LLM_Chat">LLM_Chat</a></code></h4>
</li>
<li>
<h4><code><a title="imodelsx.llm.LLM_Chat_Audio" href="#imodelsx.llm.LLM_Chat_Audio">LLM_Chat_Audio</a></code></h4>
</li>
<li>
<h4><code><a title="imodelsx.llm.LLM_HF" href="#imodelsx.llm.LLM_HF">LLM_HF</a></code></h4>
</li>
<li>
<h4><code><a title="imodelsx.llm.LLM_HF_Pipeline" href="#imodelsx.llm.LLM_HF_Pipeline">LLM_HF_Pipeline</a></code></h4>
</li>
<li>
<h4><code><a title="imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder" href="#imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder">_LazyModule.__getattr__.<locals>.Placeholder</a></code></h4>
<ul class="">
<li><code><a title="imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder.call" href="#imodelsx.llm._LazyModule.__getattr__.<locals>.Placeholder.call">call</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
