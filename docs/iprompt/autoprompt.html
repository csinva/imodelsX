<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>imodelsx.iprompt.autoprompt API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodelsx.iprompt.autoprompt</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt"><code class="flex name class">
<span>class <span class="ident">AutoPrompt</span></span>
<span>(</span><span>args: argparse.Namespace,<br>loss_func: <a title="imodelsx.iprompt.utils.PrefixLoss" href="utils.html#imodelsx.iprompt.utils.PrefixLoss">PrefixLoss</a>,<br>model: transformers.modeling_utils.PreTrainedModel,<br>tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,<br>preprefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoPrompt(HotFlip):
    args: argparse.Namespace
    loss_func: PrefixLoss
    model: transformers.PreTrainedModel
    tokenizer: transformers.PreTrainedTokenizer
    prefix_ids: torch.Tensor
    prefix_embedding: torch.nn.Parameter
    preprefix: str

    def __init__(
        self,
        args: argparse.Namespace,
        loss_func: PrefixLoss,
        model: transformers.PreTrainedModel,
        tokenizer: transformers.PreTrainedTokenizer,
        preprefix: str = &#39;&#39;
    ):
        super().__init__(
            args=args, loss_func=loss_func, model=model, tokenizer=tokenizer, preprefix=preprefix
        )
        self._do_final_reranking = args.iprompt_do_final_reranking
        # AutoPrompt-specific parameters.
        self._num_candidates_per_prefix_token = 32  # V_cand in autoprompt paper
        # This helps us know which were the best prefixes to return over time
        self._prefix_pool = PrefixPool(
            tokenizer=self.tokenizer,
            criterion=&#39;loss&#39;  # in [&#39;loss&#39;, &#39;acc&#39;, &#39;combined&#39;]
        )
        self._autoprompt_verbose = True
        self._num_min_occurrences = 1
        # Will rank and save this many prefixes at the end of training.
        self._num_prefixes_to_test = 64
    
    def test_prefixes(
        self,
        prefixes: List[Tuple[int]], 
        eval_dataloader: torch.utils.data.DataLoader, 
        possible_answer_mask: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
        prefixes at the end of training.
        &#34;&#34;&#34;
        all_candidate_losses = torch.zeros(len(prefixes), dtype=torch.float32)
        all_candidate_n_correct = torch.zeros(
            len(prefixes), dtype=torch.float32)
        total_n = 0
        for batch in tqdm.tqdm(eval_dataloader, desc=f&#39;evaluating {len(prefixes)} prefixes&#39;):
            if (self.args.n_shots &gt; 1) and (self.args.single_shot_loss): ##
               batch[&#39;input&#39;] = batch[&#39;last_input&#39;] ##
            x_text, y_text = self.prepare_batch(batch=batch)
            tok = functools.partial(
                self.tokenizer, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;,
                truncation=True, max_length=self.args.max_length  # TODO set max_length on self
            )
            x_tokenized = tok(x_text).to(device)
            y_tokenized = tok(y_text).to(device)
            total_n += len(x_tokenized.input_ids)

            next_token_ids = y_tokenized.input_ids
            for i in range(len(prefixes)):
                with torch.no_grad():
                    _cand_input_ids, cand_loss, cand_n_correct = (
                        self._compute_loss_with_set_prefix(
                            original_input_ids=x_tokenized.input_ids,
                            next_token_ids=next_token_ids,
                            possible_answer_mask=possible_answer_mask,
                            prefix_ids=torch.tensor(prefixes[i]).to(device),
                        )
                    )
                all_candidate_losses[i] += cand_loss.item()
                all_candidate_n_correct[i] += cand_n_correct.item()
        return all_candidate_losses.cpu().tolist(), (all_candidate_n_correct / total_n).cpu().tolist()

    def serialize(self, eval_dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Writes stuff to disk. Saves other stuff to save as full results file.
        &#34;&#34;&#34;

        # Uncomment following lines to save all the prefixes we tested.
        # save_dir = self.args.save_dir_unique
        # os.makedirs(save_dir, exist_ok=True)
        # pickle.dump(self._prefix_pool, open(os.path.join(save_dir, &#39;prefix_pool.p&#39;), &#39;wb&#39;))

        all_prefixes = self._prefix_pool.topk_all(
            k=self._num_prefixes_to_test, min_occurrences=3)

        if not len(all_prefixes):
            # In the case where we get no prefixes here (i.e. prompt generation
            # only ran for a single step) just take anything from prefix pool.
            all_prefixes = random.choices(list(self._prefix_pool.prefixes), k=self._num_prefixes_to_test)

        if self._do_final_reranking:
            all_losses, all_accuracies = self.test_prefixes(
                prefixes=all_prefixes,
                eval_dataloader=eval_dataloader,
                possible_answer_mask=possible_answer_mask
            )
            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
            df = df.sort_values(by=[&#39;accuracy&#39;, &#39;loss&#39;], ascending=[
                                False, True]).reset_index()
        else:
            all_prefixes = list(self._prefix_pool.prefixes)
            all_losses = [self._prefix_pool._avg_loss.get(p, -1) for p in all_prefixes]
            all_accuracies = [self._prefix_pool._avg_accuracy.get(p, -1) for p in all_prefixes]

            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
        df = df.sort_values(by=&#39;accuracy&#39;, ascending=False).reset_index()

        df[&#39;prefix_str&#39;] = df[&#39;prefix&#39;].map(self.tokenizer.decode)
        df[&#39;n_queries&#39;] = df[&#39;prefix&#39;].map(
            lambda p_ids: len(self._prefix_pool._all_losses[p_ids]))

        print(&#39;Final prefixes&#39;)
        print(df.head())

        return {
            &#34;prefix_ids&#34;: df[&#39;prefix&#39;].tolist(),
            &#34;prefixes&#34;: df[&#39;prefix_str&#39;].tolist(),
            &#34;prefix_train_acc&#34;: df[&#39;accuracy&#39;].tolist(),
            &#34;prefix_train_loss&#34;: df[&#39;loss&#39;].tolist(),
            &#34;prefix_n_queries&#34;: df[&#39;n_queries&#39;].tolist(),
        }

    def compute_loss_and_call_backward(
        self,
        x_tokenized: transformers.BatchEncoding,
        y_tokenized: transformers.BatchEncoding,
        possible_answer_mask: torch.Tensor,
        full_text_tokenized: Optional[transformers.BatchEncoding] = None
    ) -&gt; Tuple[torch.Tensor, int]:
        &#34;&#34;&#34;Computes loss using `self.loss_func`.

        Returns:
            loss (float torch.Tensor) -- the loss
            num_correct (int): number of examples where prediction was correct
        &#34;&#34;&#34;
        original_input_ids = x_tokenized.input_ids
        next_token_ids = y_tokenized.input_ids  # only compute loss over next token

        current_input_ids, current_loss, current_n_correct = self._compute_loss_with_set_prefix(
            original_input_ids=original_input_ids,
            next_token_ids=next_token_ids,
            possible_answer_mask=possible_answer_mask,
            prefix_ids=None,
        )
        current_loss.backward()

        self._autoprompt_verbose: print(
            f&#39;** {self.tokenizer.decode(self.prefix_ids)}: {current_loss:.2f}&#39;)
        
        # track running accuracy of this prefix.
        self._prefix_pool.update(
            prefix=self.prefix_ids,
            loss=current_loss,
            accuracy=(current_n_correct/len(original_input_ids))
        )

        # print an update.
        self._prefix_pool.print(topk=10, min_occurrences=1)

        #
        # Get top token replacements
        #
        token_grads = self._prefix_token_grad
        if self._is_t5:
            # t5 has extra vocab tokens for no reason:
            # https://github.com/huggingface/transformers/issues/4875#issuecomment-647634437
            assert token_grads.shape == (
                self._num_tokens, len(self.tokenizer.vocab) + 28
            )
            token_grads = token_grads[:, :-28]
        assert token_grads.shape == (
            self._num_tokens, len(self.tokenizer.vocab))
        top_tokens_per_position = (
            token_grads.topk(
                k=self._num_candidates_per_prefix_token, dim=1, largest=False).indices
        )
        assert top_tokens_per_position.shape == (
            self._num_tokens, self._num_candidates_per_prefix_token)

        top_swap_tokens = top_tokens_per_position[self._swap_token_idx, :]
        #
        # Get most likely tokens.
        #
        top_swap_tokens = token_grads.argsort(descending=False).flatten()
        top_swap_tokens = top_swap_tokens[0:
                                          self._num_candidates_per_prefix_token]

        # rank candidates
        mask = torch.nn.functional.one_hot(
            torch.tensor(self._swap_token_idx), num_classes=self._num_tokens
        ).bool().to(device)
        candidate_prefix_ids = torch.where(
            mask, top_swap_tokens[:, None], self.prefix_ids[None, :])
        is_current_prefix_mask = (
            candidate_prefix_ids == self.prefix_ids).all(dim=1)
        candidate_prefix_ids = candidate_prefix_ids[~is_current_prefix_mask]

        # get best prefix
        num_candidates = len(candidate_prefix_ids)
        all_candidate_losses = torch.zeros(
            num_candidates, dtype=float).to(device)
        all_n_correct = torch.zeros(num_candidates, dtype=int).to(device)
        for i in range(num_candidates):
            with torch.no_grad():
                cand_input_ids, cand_loss, cand_n_correct = (
                    self._compute_loss_with_set_prefix(
                        original_input_ids=original_input_ids,
                        next_token_ids=next_token_ids,
                        possible_answer_mask=possible_answer_mask,
                        prefix_ids=candidate_prefix_ids[i],
                    )
                )
            all_candidate_losses[i] = cand_loss
            all_n_correct[i] = cand_n_correct

            # self._autoprompt_verbose: print(
                # f&#39;** \t{self.tokenizer.decode(candidate_prefix_ids[i])}: {cand_loss:.2f}&#39;)

            self._prefix_pool.update(
                prefix=candidate_prefix_ids[i],
                loss=cand_loss,
                accuracy=(cand_n_correct / len(original_input_ids))
            )

        # randomly change the token to swap
        self._swap_token_idx = random.randint(0, (self._num_tokens-1))
        # get best prefix we&#39;ve seen
        if all_candidate_losses.min() &lt; current_loss:
            best_prefix = candidate_prefix_ids[all_candidate_losses.argmin()]
            best_prefix_loss = all_candidate_losses.min()
            best_prefix_n_correct = all_n_correct[all_candidate_losses.argmin(
            )]
            if self._autoprompt_verbose:
                print(&#34;** set new prefix&#34;, best_prefix)
        else:
            best_prefix = self.prefix_ids
            best_prefix_loss = current_loss
            best_prefix_n_correct = current_n_correct
            if self._autoprompt_verbose:
                print(&#34;** set same prefix&#34;, best_prefix)

        self._set_prefix_ids(best_prefix)
        return best_prefix_loss, best_prefix_n_correct

    def post_epoch(self, dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; None:
        #
        # Get candidate IDs for every position.
        #
        pass</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodelsx.iprompt.hotflip.HotFlip" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip">HotFlip</a></li>
<li><a title="imodelsx.iprompt.utils.PrefixModel" href="utils.html#imodelsx.iprompt.utils.PrefixModel">PrefixModel</a></li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodelsx.iprompt.ipromptx.iPrompt" href="ipromptx.html#imodelsx.iprompt.ipromptx.iPrompt">iPrompt</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.args"><code class="name">var <span class="ident">args</span> : argparse.Namespace</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.loss_func"><code class="name">var <span class="ident">loss_func</span> : <a title="imodelsx.iprompt.utils.PrefixLoss" href="utils.html#imodelsx.iprompt.utils.PrefixLoss">PrefixLoss</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.model"><code class="name">var <span class="ident">model</span> : transformers.modeling_utils.PreTrainedModel</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding"><code class="name">var <span class="ident">prefix_embedding</span> : torch.nn.parameter.Parameter</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids"><code class="name">var <span class="ident">prefix_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.preprefix"><code class="name">var <span class="ident">preprefix</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer"><code class="name">var <span class="ident">tokenizer</span> : transformers.tokenization_utils.PreTrainedTokenizer</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch"><code class="name flex">
<span>def <span class="ident">post_epoch</span></span>(<span>self,<br>dataloader: torch.utils.data.dataloader.DataLoader,<br>possible_answer_mask: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_epoch(self, dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; None:
    #
    # Get candidate IDs for every position.
    #
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self,<br>eval_dataloader: torch.utils.data.dataloader.DataLoader,<br>possible_answer_mask: torch.Tensor) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, eval_dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Writes stuff to disk. Saves other stuff to save as full results file.
    &#34;&#34;&#34;

    # Uncomment following lines to save all the prefixes we tested.
    # save_dir = self.args.save_dir_unique
    # os.makedirs(save_dir, exist_ok=True)
    # pickle.dump(self._prefix_pool, open(os.path.join(save_dir, &#39;prefix_pool.p&#39;), &#39;wb&#39;))

    all_prefixes = self._prefix_pool.topk_all(
        k=self._num_prefixes_to_test, min_occurrences=3)

    if not len(all_prefixes):
        # In the case where we get no prefixes here (i.e. prompt generation
        # only ran for a single step) just take anything from prefix pool.
        all_prefixes = random.choices(list(self._prefix_pool.prefixes), k=self._num_prefixes_to_test)

    if self._do_final_reranking:
        all_losses, all_accuracies = self.test_prefixes(
            prefixes=all_prefixes,
            eval_dataloader=eval_dataloader,
            possible_answer_mask=possible_answer_mask
        )
        df = pd.DataFrame(
            zip(*[all_prefixes, all_losses, all_accuracies]),
            columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
        )
        df = df.sort_values(by=[&#39;accuracy&#39;, &#39;loss&#39;], ascending=[
                            False, True]).reset_index()
    else:
        all_prefixes = list(self._prefix_pool.prefixes)
        all_losses = [self._prefix_pool._avg_loss.get(p, -1) for p in all_prefixes]
        all_accuracies = [self._prefix_pool._avg_accuracy.get(p, -1) for p in all_prefixes]

        df = pd.DataFrame(
            zip(*[all_prefixes, all_losses, all_accuracies]),
            columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
        )
    df = df.sort_values(by=&#39;accuracy&#39;, ascending=False).reset_index()

    df[&#39;prefix_str&#39;] = df[&#39;prefix&#39;].map(self.tokenizer.decode)
    df[&#39;n_queries&#39;] = df[&#39;prefix&#39;].map(
        lambda p_ids: len(self._prefix_pool._all_losses[p_ids]))

    print(&#39;Final prefixes&#39;)
    print(df.head())

    return {
        &#34;prefix_ids&#34;: df[&#39;prefix&#39;].tolist(),
        &#34;prefixes&#34;: df[&#39;prefix_str&#39;].tolist(),
        &#34;prefix_train_acc&#34;: df[&#39;accuracy&#39;].tolist(),
        &#34;prefix_train_loss&#34;: df[&#39;loss&#39;].tolist(),
        &#34;prefix_n_queries&#34;: df[&#39;n_queries&#39;].tolist(),
    }</code></pre>
</details>
<div class="desc"><p>Writes stuff to disk. Saves other stuff to save as full results file.</p></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes"><code class="name flex">
<span>def <span class="ident">test_prefixes</span></span>(<span>self,<br>prefixes: List[Tuple[int]],<br>eval_dataloader: torch.utils.data.dataloader.DataLoader,<br>possible_answer_mask: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_prefixes(
    self,
    prefixes: List[Tuple[int]], 
    eval_dataloader: torch.utils.data.DataLoader, 
    possible_answer_mask: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
    prefixes at the end of training.
    &#34;&#34;&#34;
    all_candidate_losses = torch.zeros(len(prefixes), dtype=torch.float32)
    all_candidate_n_correct = torch.zeros(
        len(prefixes), dtype=torch.float32)
    total_n = 0
    for batch in tqdm.tqdm(eval_dataloader, desc=f&#39;evaluating {len(prefixes)} prefixes&#39;):
        if (self.args.n_shots &gt; 1) and (self.args.single_shot_loss): ##
           batch[&#39;input&#39;] = batch[&#39;last_input&#39;] ##
        x_text, y_text = self.prepare_batch(batch=batch)
        tok = functools.partial(
            self.tokenizer, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;,
            truncation=True, max_length=self.args.max_length  # TODO set max_length on self
        )
        x_tokenized = tok(x_text).to(device)
        y_tokenized = tok(y_text).to(device)
        total_n += len(x_tokenized.input_ids)

        next_token_ids = y_tokenized.input_ids
        for i in range(len(prefixes)):
            with torch.no_grad():
                _cand_input_ids, cand_loss, cand_n_correct = (
                    self._compute_loss_with_set_prefix(
                        original_input_ids=x_tokenized.input_ids,
                        next_token_ids=next_token_ids,
                        possible_answer_mask=possible_answer_mask,
                        prefix_ids=torch.tensor(prefixes[i]).to(device),
                    )
                )
            all_candidate_losses[i] += cand_loss.item()
            all_candidate_n_correct[i] += cand_n_correct.item()
    return all_candidate_losses.cpu().tolist(), (all_candidate_n_correct / total_n).cpu().tolist()</code></pre>
</details>
<div class="desc"><p>Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
prefixes at the end of training.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imodelsx.iprompt.hotflip.HotFlip" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip">HotFlip</a></b></code>:
<ul class="hlist">
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.check_early_stop" href="utils.html#imodelsx.iprompt.utils.PrefixModel.check_early_stop">check_early_stop</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.compute_loss_and_call_backward" href="utils.html#imodelsx.iprompt.utils.PrefixModel.compute_loss_and_call_backward">compute_loss_and_call_backward</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.embed_input_ids" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip.embed_input_ids">embed_input_ids</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.forward" href="utils.html#imodelsx.iprompt.utils.PrefixModel.forward">forward</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.prepare_batch" href="utils.html#imodelsx.iprompt.utils.PrefixModel.prepare_batch">prepare_batch</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodelsx.iprompt" href="index.html">imodelsx.iprompt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt" href="#imodelsx.iprompt.autoprompt.AutoPrompt">AutoPrompt</a></code></h4>
<ul class="two-column">
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.args" href="#imodelsx.iprompt.autoprompt.AutoPrompt.args">args</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.loss_func" href="#imodelsx.iprompt.autoprompt.AutoPrompt.loss_func">loss_func</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.model" href="#imodelsx.iprompt.autoprompt.AutoPrompt.model">model</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch" href="#imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch">post_epoch</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding" href="#imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding">prefix_embedding</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids" href="#imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids">prefix_ids</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.preprefix" href="#imodelsx.iprompt.autoprompt.AutoPrompt.preprefix">preprefix</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.serialize" href="#imodelsx.iprompt.autoprompt.AutoPrompt.serialize">serialize</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes" href="#imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes">test_prefixes</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer" href="#imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer">tokenizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
