<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imodelsx.iprompt.autoprompt API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodelsx.iprompt.autoprompt</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Any, Dict, List, Optional, Tuple

import argparse
import functools
import os
import pickle
import random

import pandas as pd
import torch
import tqdm
import transformers

from .hotflip import HotFlip
from .utils import device, PrefixLoss, PrefixModel, PrefixPool


class AutoPrompt(HotFlip):
    args: argparse.Namespace
    loss_func: PrefixLoss
    model: transformers.PreTrainedModel
    tokenizer: transformers.PreTrainedTokenizer
    prefix_ids: torch.Tensor
    prefix_embedding: torch.nn.Parameter
    preprefix: str

    def __init__(
        self,
        args: argparse.Namespace,
        loss_func: PrefixLoss,
        model: transformers.PreTrainedModel,
        tokenizer: transformers.PreTrainedTokenizer,
        preprefix: str = &#39;&#39;
    ):
        super().__init__(
            args=args, loss_func=loss_func, model=model, tokenizer=tokenizer, preprefix=preprefix
        )
        self._do_final_reranking = args.iprompt_do_final_reranking
        # AutoPrompt-specific parameters.
        self._num_candidates_per_prefix_token = 32  # V_cand in autoprompt paper
        # This helps us know which were the best prefixes to return over time
        self._prefix_pool = PrefixPool(
            tokenizer=self.tokenizer,
            criterion=&#39;loss&#39;  # in [&#39;loss&#39;, &#39;acc&#39;, &#39;combined&#39;]
        )
        self._autoprompt_verbose = True
        self._num_min_occurrences = 1
        # Will rank and save this many prefixes at the end of training.
        self._num_prefixes_to_test = 64
    
    def test_prefixes(
        self,
        prefixes: List[Tuple[int]], 
        eval_dataloader: torch.utils.data.DataLoader, 
        possible_answer_mask: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
        prefixes at the end of training.
        &#34;&#34;&#34;
        all_candidate_losses = torch.zeros(len(prefixes), dtype=torch.float32)
        all_candidate_n_correct = torch.zeros(
            len(prefixes), dtype=torch.float32)
        total_n = 0
        for batch in tqdm.tqdm(eval_dataloader, desc=f&#39;evaluating {len(prefixes)} prefixes&#39;):
            if (self.args.n_shots &gt; 1) and (self.args.single_shot_loss): ##
               batch[&#39;input&#39;] = batch[&#39;last_input&#39;] ##
            x_text, y_text = self.prepare_batch(batch=batch)
            tok = functools.partial(
                self.tokenizer, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;,
                truncation=True, max_length=self.args.max_length  # TODO set max_length on self
            )
            x_tokenized = tok(x_text).to(device)
            y_tokenized = tok(y_text).to(device)
            total_n += len(x_tokenized.input_ids)

            next_token_ids = y_tokenized.input_ids
            for i in range(len(prefixes)):
                with torch.no_grad():
                    _cand_input_ids, cand_loss, cand_n_correct = (
                        self._compute_loss_with_set_prefix(
                            original_input_ids=x_tokenized.input_ids,
                            next_token_ids=next_token_ids,
                            possible_answer_mask=possible_answer_mask,
                            prefix_ids=torch.tensor(prefixes[i]).to(device),
                        )
                    )
                all_candidate_losses[i] += cand_loss.item()
                all_candidate_n_correct[i] += cand_n_correct.item()
        return all_candidate_losses.cpu().tolist(), (all_candidate_n_correct / total_n).cpu().tolist()

    def serialize(self, eval_dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Writes stuff to disk. Saves other stuff to save as full results file.
        &#34;&#34;&#34;

        # Uncomment following lines to save all the prefixes we tested.
        # save_dir = self.args.save_dir_unique
        # os.makedirs(save_dir, exist_ok=True)
        # pickle.dump(self._prefix_pool, open(os.path.join(save_dir, &#39;prefix_pool.p&#39;), &#39;wb&#39;))

        all_prefixes = self._prefix_pool.topk_all(
            k=self._num_prefixes_to_test, min_occurrences=3)

        if not len(all_prefixes):
            # In the case where we get no prefixes here (i.e. prompt generation
            # only ran for a single step) just take anything from prefix pool.
            all_prefixes = random.choices(list(self._prefix_pool.prefixes), k=self._num_prefixes_to_test)

        if self._do_final_reranking:
            all_losses, all_accuracies = self.test_prefixes(
                prefixes=all_prefixes,
                eval_dataloader=eval_dataloader,
                possible_answer_mask=possible_answer_mask
            )
            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
            df = df.sort_values(by=[&#39;accuracy&#39;, &#39;loss&#39;], ascending=[
                                False, True]).reset_index()
        else:
            all_prefixes = list(self._prefix_pool.prefixes)
            all_losses = [self._prefix_pool._avg_loss.get(p, -1) for p in all_prefixes]
            all_accuracies = [self._prefix_pool._avg_accuracy.get(p, -1) for p in all_prefixes]

            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
        df = df.sort_values(by=&#39;accuracy&#39;, ascending=False).reset_index()

        df[&#39;prefix_str&#39;] = df[&#39;prefix&#39;].map(self.tokenizer.decode)
        df[&#39;n_queries&#39;] = df[&#39;prefix&#39;].map(
            lambda p_ids: len(self._prefix_pool._all_losses[p_ids]))

        print(&#39;Final prefixes&#39;)
        print(df.head())

        return {
            &#34;prefix_ids&#34;: df[&#39;prefix&#39;].tolist(),
            &#34;prefixes&#34;: df[&#39;prefix_str&#39;].tolist(),
            &#34;prefix_train_acc&#34;: df[&#39;accuracy&#39;].tolist(),
            &#34;prefix_train_loss&#34;: df[&#39;loss&#39;].tolist(),
            &#34;prefix_n_queries&#34;: df[&#39;n_queries&#39;].tolist(),
        }

    def compute_loss_and_call_backward(
        self,
        x_tokenized: transformers.BatchEncoding,
        y_tokenized: transformers.BatchEncoding,
        possible_answer_mask: torch.Tensor,
        full_text_tokenized: Optional[transformers.BatchEncoding] = None
    ) -&gt; Tuple[torch.Tensor, int]:
        &#34;&#34;&#34;Computes loss using `self.loss_func`.

        Returns:
            loss (float torch.Tensor) -- the loss
            num_correct (int): number of examples where prediction was correct
        &#34;&#34;&#34;
        original_input_ids = x_tokenized.input_ids
        next_token_ids = y_tokenized.input_ids  # only compute loss over next token

        current_input_ids, current_loss, current_n_correct = self._compute_loss_with_set_prefix(
            original_input_ids=original_input_ids,
            next_token_ids=next_token_ids,
            possible_answer_mask=possible_answer_mask,
            prefix_ids=None,
        )
        current_loss.backward()

        self._autoprompt_verbose: print(
            f&#39;** {self.tokenizer.decode(self.prefix_ids)}: {current_loss:.2f}&#39;)
        
        # track running accuracy of this prefix.
        self._prefix_pool.update(
            prefix=self.prefix_ids,
            loss=current_loss,
            accuracy=(current_n_correct/len(original_input_ids))
        )

        # print an update.
        self._prefix_pool.print(topk=10, min_occurrences=1)

        #
        # Get top token replacements
        #
        token_grads = self._prefix_token_grad
        if self._is_t5:
            # t5 has extra vocab tokens for no reason:
            # https://github.com/huggingface/transformers/issues/4875#issuecomment-647634437
            assert token_grads.shape == (
                self._num_tokens, len(self.tokenizer.vocab) + 28
            )
            token_grads = token_grads[:, :-28]
        assert token_grads.shape == (
            self._num_tokens, len(self.tokenizer.vocab))
        top_tokens_per_position = (
            token_grads.topk(
                k=self._num_candidates_per_prefix_token, dim=1, largest=False).indices
        )
        assert top_tokens_per_position.shape == (
            self._num_tokens, self._num_candidates_per_prefix_token)

        top_swap_tokens = top_tokens_per_position[self._swap_token_idx, :]
        #
        # Get most likely tokens.
        #
        top_swap_tokens = token_grads.argsort(descending=False).flatten()
        top_swap_tokens = top_swap_tokens[0:
                                          self._num_candidates_per_prefix_token]

        # rank candidates
        mask = torch.nn.functional.one_hot(
            torch.tensor(self._swap_token_idx), num_classes=self._num_tokens
        ).bool().to(device)
        candidate_prefix_ids = torch.where(
            mask, top_swap_tokens[:, None], self.prefix_ids[None, :])
        is_current_prefix_mask = (
            candidate_prefix_ids == self.prefix_ids).all(dim=1)
        candidate_prefix_ids = candidate_prefix_ids[~is_current_prefix_mask]

        # get best prefix
        num_candidates = len(candidate_prefix_ids)
        all_candidate_losses = torch.zeros(
            num_candidates, dtype=float).to(device)
        all_n_correct = torch.zeros(num_candidates, dtype=int).to(device)
        for i in range(num_candidates):
            with torch.no_grad():
                cand_input_ids, cand_loss, cand_n_correct = (
                    self._compute_loss_with_set_prefix(
                        original_input_ids=original_input_ids,
                        next_token_ids=next_token_ids,
                        possible_answer_mask=possible_answer_mask,
                        prefix_ids=candidate_prefix_ids[i],
                    )
                )
            all_candidate_losses[i] = cand_loss
            all_n_correct[i] = cand_n_correct

            # self._autoprompt_verbose: print(
                # f&#39;** \t{self.tokenizer.decode(candidate_prefix_ids[i])}: {cand_loss:.2f}&#39;)

            self._prefix_pool.update(
                prefix=candidate_prefix_ids[i],
                loss=cand_loss,
                accuracy=(cand_n_correct / len(original_input_ids))
            )

        # randomly change the token to swap
        self._swap_token_idx = random.randint(0, (self._num_tokens-1))
        # get best prefix we&#39;ve seen
        if all_candidate_losses.min() &lt; current_loss:
            best_prefix = candidate_prefix_ids[all_candidate_losses.argmin()]
            best_prefix_loss = all_candidate_losses.min()
            best_prefix_n_correct = all_n_correct[all_candidate_losses.argmin(
            )]
            if self._autoprompt_verbose:
                print(&#34;** set new prefix&#34;, best_prefix)
        else:
            best_prefix = self.prefix_ids
            best_prefix_loss = current_loss
            best_prefix_n_correct = current_n_correct
            if self._autoprompt_verbose:
                print(&#34;** set same prefix&#34;, best_prefix)

        self._set_prefix_ids(best_prefix)
        return best_prefix_loss, best_prefix_n_correct

    def post_epoch(self, dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; None:
        #
        # Get candidate IDs for every position.
        #
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt"><code class="flex name class">
<span>class <span class="ident">AutoPrompt</span></span>
<span>(</span><span>args: argparse.Namespace, loss_func: <a title="imodelsx.iprompt.utils.PrefixLoss" href="utils.html#imodelsx.iprompt.utils.PrefixLoss">PrefixLoss</a>, model: transformers.modeling_utils.PreTrainedModel, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, preprefix: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoPrompt(HotFlip):
    args: argparse.Namespace
    loss_func: PrefixLoss
    model: transformers.PreTrainedModel
    tokenizer: transformers.PreTrainedTokenizer
    prefix_ids: torch.Tensor
    prefix_embedding: torch.nn.Parameter
    preprefix: str

    def __init__(
        self,
        args: argparse.Namespace,
        loss_func: PrefixLoss,
        model: transformers.PreTrainedModel,
        tokenizer: transformers.PreTrainedTokenizer,
        preprefix: str = &#39;&#39;
    ):
        super().__init__(
            args=args, loss_func=loss_func, model=model, tokenizer=tokenizer, preprefix=preprefix
        )
        self._do_final_reranking = args.iprompt_do_final_reranking
        # AutoPrompt-specific parameters.
        self._num_candidates_per_prefix_token = 32  # V_cand in autoprompt paper
        # This helps us know which were the best prefixes to return over time
        self._prefix_pool = PrefixPool(
            tokenizer=self.tokenizer,
            criterion=&#39;loss&#39;  # in [&#39;loss&#39;, &#39;acc&#39;, &#39;combined&#39;]
        )
        self._autoprompt_verbose = True
        self._num_min_occurrences = 1
        # Will rank and save this many prefixes at the end of training.
        self._num_prefixes_to_test = 64
    
    def test_prefixes(
        self,
        prefixes: List[Tuple[int]], 
        eval_dataloader: torch.utils.data.DataLoader, 
        possible_answer_mask: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
        prefixes at the end of training.
        &#34;&#34;&#34;
        all_candidate_losses = torch.zeros(len(prefixes), dtype=torch.float32)
        all_candidate_n_correct = torch.zeros(
            len(prefixes), dtype=torch.float32)
        total_n = 0
        for batch in tqdm.tqdm(eval_dataloader, desc=f&#39;evaluating {len(prefixes)} prefixes&#39;):
            if (self.args.n_shots &gt; 1) and (self.args.single_shot_loss): ##
               batch[&#39;input&#39;] = batch[&#39;last_input&#39;] ##
            x_text, y_text = self.prepare_batch(batch=batch)
            tok = functools.partial(
                self.tokenizer, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;,
                truncation=True, max_length=self.args.max_length  # TODO set max_length on self
            )
            x_tokenized = tok(x_text).to(device)
            y_tokenized = tok(y_text).to(device)
            total_n += len(x_tokenized.input_ids)

            next_token_ids = y_tokenized.input_ids
            for i in range(len(prefixes)):
                with torch.no_grad():
                    _cand_input_ids, cand_loss, cand_n_correct = (
                        self._compute_loss_with_set_prefix(
                            original_input_ids=x_tokenized.input_ids,
                            next_token_ids=next_token_ids,
                            possible_answer_mask=possible_answer_mask,
                            prefix_ids=torch.tensor(prefixes[i]).to(device),
                        )
                    )
                all_candidate_losses[i] += cand_loss.item()
                all_candidate_n_correct[i] += cand_n_correct.item()
        return all_candidate_losses.cpu().tolist(), (all_candidate_n_correct / total_n).cpu().tolist()

    def serialize(self, eval_dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Writes stuff to disk. Saves other stuff to save as full results file.
        &#34;&#34;&#34;

        # Uncomment following lines to save all the prefixes we tested.
        # save_dir = self.args.save_dir_unique
        # os.makedirs(save_dir, exist_ok=True)
        # pickle.dump(self._prefix_pool, open(os.path.join(save_dir, &#39;prefix_pool.p&#39;), &#39;wb&#39;))

        all_prefixes = self._prefix_pool.topk_all(
            k=self._num_prefixes_to_test, min_occurrences=3)

        if not len(all_prefixes):
            # In the case where we get no prefixes here (i.e. prompt generation
            # only ran for a single step) just take anything from prefix pool.
            all_prefixes = random.choices(list(self._prefix_pool.prefixes), k=self._num_prefixes_to_test)

        if self._do_final_reranking:
            all_losses, all_accuracies = self.test_prefixes(
                prefixes=all_prefixes,
                eval_dataloader=eval_dataloader,
                possible_answer_mask=possible_answer_mask
            )
            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
            df = df.sort_values(by=[&#39;accuracy&#39;, &#39;loss&#39;], ascending=[
                                False, True]).reset_index()
        else:
            all_prefixes = list(self._prefix_pool.prefixes)
            all_losses = [self._prefix_pool._avg_loss.get(p, -1) for p in all_prefixes]
            all_accuracies = [self._prefix_pool._avg_accuracy.get(p, -1) for p in all_prefixes]

            df = pd.DataFrame(
                zip(*[all_prefixes, all_losses, all_accuracies]),
                columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
            )
        df = df.sort_values(by=&#39;accuracy&#39;, ascending=False).reset_index()

        df[&#39;prefix_str&#39;] = df[&#39;prefix&#39;].map(self.tokenizer.decode)
        df[&#39;n_queries&#39;] = df[&#39;prefix&#39;].map(
            lambda p_ids: len(self._prefix_pool._all_losses[p_ids]))

        print(&#39;Final prefixes&#39;)
        print(df.head())

        return {
            &#34;prefix_ids&#34;: df[&#39;prefix&#39;].tolist(),
            &#34;prefixes&#34;: df[&#39;prefix_str&#39;].tolist(),
            &#34;prefix_train_acc&#34;: df[&#39;accuracy&#39;].tolist(),
            &#34;prefix_train_loss&#34;: df[&#39;loss&#39;].tolist(),
            &#34;prefix_n_queries&#34;: df[&#39;n_queries&#39;].tolist(),
        }

    def compute_loss_and_call_backward(
        self,
        x_tokenized: transformers.BatchEncoding,
        y_tokenized: transformers.BatchEncoding,
        possible_answer_mask: torch.Tensor,
        full_text_tokenized: Optional[transformers.BatchEncoding] = None
    ) -&gt; Tuple[torch.Tensor, int]:
        &#34;&#34;&#34;Computes loss using `self.loss_func`.

        Returns:
            loss (float torch.Tensor) -- the loss
            num_correct (int): number of examples where prediction was correct
        &#34;&#34;&#34;
        original_input_ids = x_tokenized.input_ids
        next_token_ids = y_tokenized.input_ids  # only compute loss over next token

        current_input_ids, current_loss, current_n_correct = self._compute_loss_with_set_prefix(
            original_input_ids=original_input_ids,
            next_token_ids=next_token_ids,
            possible_answer_mask=possible_answer_mask,
            prefix_ids=None,
        )
        current_loss.backward()

        self._autoprompt_verbose: print(
            f&#39;** {self.tokenizer.decode(self.prefix_ids)}: {current_loss:.2f}&#39;)
        
        # track running accuracy of this prefix.
        self._prefix_pool.update(
            prefix=self.prefix_ids,
            loss=current_loss,
            accuracy=(current_n_correct/len(original_input_ids))
        )

        # print an update.
        self._prefix_pool.print(topk=10, min_occurrences=1)

        #
        # Get top token replacements
        #
        token_grads = self._prefix_token_grad
        if self._is_t5:
            # t5 has extra vocab tokens for no reason:
            # https://github.com/huggingface/transformers/issues/4875#issuecomment-647634437
            assert token_grads.shape == (
                self._num_tokens, len(self.tokenizer.vocab) + 28
            )
            token_grads = token_grads[:, :-28]
        assert token_grads.shape == (
            self._num_tokens, len(self.tokenizer.vocab))
        top_tokens_per_position = (
            token_grads.topk(
                k=self._num_candidates_per_prefix_token, dim=1, largest=False).indices
        )
        assert top_tokens_per_position.shape == (
            self._num_tokens, self._num_candidates_per_prefix_token)

        top_swap_tokens = top_tokens_per_position[self._swap_token_idx, :]
        #
        # Get most likely tokens.
        #
        top_swap_tokens = token_grads.argsort(descending=False).flatten()
        top_swap_tokens = top_swap_tokens[0:
                                          self._num_candidates_per_prefix_token]

        # rank candidates
        mask = torch.nn.functional.one_hot(
            torch.tensor(self._swap_token_idx), num_classes=self._num_tokens
        ).bool().to(device)
        candidate_prefix_ids = torch.where(
            mask, top_swap_tokens[:, None], self.prefix_ids[None, :])
        is_current_prefix_mask = (
            candidate_prefix_ids == self.prefix_ids).all(dim=1)
        candidate_prefix_ids = candidate_prefix_ids[~is_current_prefix_mask]

        # get best prefix
        num_candidates = len(candidate_prefix_ids)
        all_candidate_losses = torch.zeros(
            num_candidates, dtype=float).to(device)
        all_n_correct = torch.zeros(num_candidates, dtype=int).to(device)
        for i in range(num_candidates):
            with torch.no_grad():
                cand_input_ids, cand_loss, cand_n_correct = (
                    self._compute_loss_with_set_prefix(
                        original_input_ids=original_input_ids,
                        next_token_ids=next_token_ids,
                        possible_answer_mask=possible_answer_mask,
                        prefix_ids=candidate_prefix_ids[i],
                    )
                )
            all_candidate_losses[i] = cand_loss
            all_n_correct[i] = cand_n_correct

            # self._autoprompt_verbose: print(
                # f&#39;** \t{self.tokenizer.decode(candidate_prefix_ids[i])}: {cand_loss:.2f}&#39;)

            self._prefix_pool.update(
                prefix=candidate_prefix_ids[i],
                loss=cand_loss,
                accuracy=(cand_n_correct / len(original_input_ids))
            )

        # randomly change the token to swap
        self._swap_token_idx = random.randint(0, (self._num_tokens-1))
        # get best prefix we&#39;ve seen
        if all_candidate_losses.min() &lt; current_loss:
            best_prefix = candidate_prefix_ids[all_candidate_losses.argmin()]
            best_prefix_loss = all_candidate_losses.min()
            best_prefix_n_correct = all_n_correct[all_candidate_losses.argmin(
            )]
            if self._autoprompt_verbose:
                print(&#34;** set new prefix&#34;, best_prefix)
        else:
            best_prefix = self.prefix_ids
            best_prefix_loss = current_loss
            best_prefix_n_correct = current_n_correct
            if self._autoprompt_verbose:
                print(&#34;** set same prefix&#34;, best_prefix)

        self._set_prefix_ids(best_prefix)
        return best_prefix_loss, best_prefix_n_correct

    def post_epoch(self, dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; None:
        #
        # Get candidate IDs for every position.
        #
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodelsx.iprompt.hotflip.HotFlip" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip">HotFlip</a></li>
<li><a title="imodelsx.iprompt.utils.PrefixModel" href="utils.html#imodelsx.iprompt.utils.PrefixModel">PrefixModel</a></li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodelsx.iprompt.ipromptx.iPrompt" href="ipromptx.html#imodelsx.iprompt.ipromptx.iPrompt">iPrompt</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.args"><code class="name">var <span class="ident">args</span> : argparse.Namespace</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.loss_func"><code class="name">var <span class="ident">loss_func</span> : <a title="imodelsx.iprompt.utils.PrefixLoss" href="utils.html#imodelsx.iprompt.utils.PrefixLoss">PrefixLoss</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.model"><code class="name">var <span class="ident">model</span> : transformers.modeling_utils.PreTrainedModel</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding"><code class="name">var <span class="ident">prefix_embedding</span> : torch.nn.parameter.Parameter</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids"><code class="name">var <span class="ident">prefix_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.preprefix"><code class="name">var <span class="ident">preprefix</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer"><code class="name">var <span class="ident">tokenizer</span> : transformers.tokenization_utils.PreTrainedTokenizer</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch"><code class="name flex">
<span>def <span class="ident">post_epoch</span></span>(<span>self, dataloader: torch.utils.data.dataloader.DataLoader, possible_answer_mask: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_epoch(self, dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; None:
    #
    # Get candidate IDs for every position.
    #
    pass</code></pre>
</details>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self, eval_dataloader: torch.utils.data.dataloader.DataLoader, possible_answer_mask: torch.Tensor) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Writes stuff to disk. Saves other stuff to save as full results file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, eval_dataloader: torch.utils.data.DataLoader, possible_answer_mask: torch.Tensor) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Writes stuff to disk. Saves other stuff to save as full results file.
    &#34;&#34;&#34;

    # Uncomment following lines to save all the prefixes we tested.
    # save_dir = self.args.save_dir_unique
    # os.makedirs(save_dir, exist_ok=True)
    # pickle.dump(self._prefix_pool, open(os.path.join(save_dir, &#39;prefix_pool.p&#39;), &#39;wb&#39;))

    all_prefixes = self._prefix_pool.topk_all(
        k=self._num_prefixes_to_test, min_occurrences=3)

    if not len(all_prefixes):
        # In the case where we get no prefixes here (i.e. prompt generation
        # only ran for a single step) just take anything from prefix pool.
        all_prefixes = random.choices(list(self._prefix_pool.prefixes), k=self._num_prefixes_to_test)

    if self._do_final_reranking:
        all_losses, all_accuracies = self.test_prefixes(
            prefixes=all_prefixes,
            eval_dataloader=eval_dataloader,
            possible_answer_mask=possible_answer_mask
        )
        df = pd.DataFrame(
            zip(*[all_prefixes, all_losses, all_accuracies]),
            columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
        )
        df = df.sort_values(by=[&#39;accuracy&#39;, &#39;loss&#39;], ascending=[
                            False, True]).reset_index()
    else:
        all_prefixes = list(self._prefix_pool.prefixes)
        all_losses = [self._prefix_pool._avg_loss.get(p, -1) for p in all_prefixes]
        all_accuracies = [self._prefix_pool._avg_accuracy.get(p, -1) for p in all_prefixes]

        df = pd.DataFrame(
            zip(*[all_prefixes, all_losses, all_accuracies]),
            columns=[&#39;prefix&#39;, &#39;loss&#39;, &#39;accuracy&#39;]
        )
    df = df.sort_values(by=&#39;accuracy&#39;, ascending=False).reset_index()

    df[&#39;prefix_str&#39;] = df[&#39;prefix&#39;].map(self.tokenizer.decode)
    df[&#39;n_queries&#39;] = df[&#39;prefix&#39;].map(
        lambda p_ids: len(self._prefix_pool._all_losses[p_ids]))

    print(&#39;Final prefixes&#39;)
    print(df.head())

    return {
        &#34;prefix_ids&#34;: df[&#39;prefix&#39;].tolist(),
        &#34;prefixes&#34;: df[&#39;prefix_str&#39;].tolist(),
        &#34;prefix_train_acc&#34;: df[&#39;accuracy&#39;].tolist(),
        &#34;prefix_train_loss&#34;: df[&#39;loss&#39;].tolist(),
        &#34;prefix_n_queries&#34;: df[&#39;n_queries&#39;].tolist(),
    }</code></pre>
</details>
</dd>
<dt id="imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes"><code class="name flex">
<span>def <span class="ident">test_prefixes</span></span>(<span>self, prefixes: List[Tuple[int]], eval_dataloader: torch.utils.data.dataloader.DataLoader, possible_answer_mask: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
prefixes at the end of training.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_prefixes(
    self,
    prefixes: List[Tuple[int]], 
    eval_dataloader: torch.utils.data.DataLoader, 
    possible_answer_mask: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Computes loss &amp; accuracy for each prefix on data in dataloader. Used to rank
    prefixes at the end of training.
    &#34;&#34;&#34;
    all_candidate_losses = torch.zeros(len(prefixes), dtype=torch.float32)
    all_candidate_n_correct = torch.zeros(
        len(prefixes), dtype=torch.float32)
    total_n = 0
    for batch in tqdm.tqdm(eval_dataloader, desc=f&#39;evaluating {len(prefixes)} prefixes&#39;):
        if (self.args.n_shots &gt; 1) and (self.args.single_shot_loss): ##
           batch[&#39;input&#39;] = batch[&#39;last_input&#39;] ##
        x_text, y_text = self.prepare_batch(batch=batch)
        tok = functools.partial(
            self.tokenizer, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;,
            truncation=True, max_length=self.args.max_length  # TODO set max_length on self
        )
        x_tokenized = tok(x_text).to(device)
        y_tokenized = tok(y_text).to(device)
        total_n += len(x_tokenized.input_ids)

        next_token_ids = y_tokenized.input_ids
        for i in range(len(prefixes)):
            with torch.no_grad():
                _cand_input_ids, cand_loss, cand_n_correct = (
                    self._compute_loss_with_set_prefix(
                        original_input_ids=x_tokenized.input_ids,
                        next_token_ids=next_token_ids,
                        possible_answer_mask=possible_answer_mask,
                        prefix_ids=torch.tensor(prefixes[i]).to(device),
                    )
                )
            all_candidate_losses[i] += cand_loss.item()
            all_candidate_n_correct[i] += cand_n_correct.item()
    return all_candidate_losses.cpu().tolist(), (all_candidate_n_correct / total_n).cpu().tolist()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imodelsx.iprompt.hotflip.HotFlip" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip">HotFlip</a></b></code>:
<ul class="hlist">
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.check_early_stop" href="utils.html#imodelsx.iprompt.utils.PrefixModel.check_early_stop">check_early_stop</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.compute_loss_and_call_backward" href="utils.html#imodelsx.iprompt.utils.PrefixModel.compute_loss_and_call_backward">compute_loss_and_call_backward</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.embed_input_ids" href="hotflip.html#imodelsx.iprompt.hotflip.HotFlip.embed_input_ids">embed_input_ids</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.forward" href="utils.html#imodelsx.iprompt.utils.PrefixModel.forward">forward</a></code></li>
<li><code><a title="imodelsx.iprompt.hotflip.HotFlip.prepare_batch" href="utils.html#imodelsx.iprompt.utils.PrefixModel.prepare_batch">prepare_batch</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodelsx.iprompt" href="index.html">imodelsx.iprompt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt" href="#imodelsx.iprompt.autoprompt.AutoPrompt">AutoPrompt</a></code></h4>
<ul class="two-column">
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.args" href="#imodelsx.iprompt.autoprompt.AutoPrompt.args">args</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.loss_func" href="#imodelsx.iprompt.autoprompt.AutoPrompt.loss_func">loss_func</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.model" href="#imodelsx.iprompt.autoprompt.AutoPrompt.model">model</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch" href="#imodelsx.iprompt.autoprompt.AutoPrompt.post_epoch">post_epoch</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding" href="#imodelsx.iprompt.autoprompt.AutoPrompt.prefix_embedding">prefix_embedding</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids" href="#imodelsx.iprompt.autoprompt.AutoPrompt.prefix_ids">prefix_ids</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.preprefix" href="#imodelsx.iprompt.autoprompt.AutoPrompt.preprefix">preprefix</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.serialize" href="#imodelsx.iprompt.autoprompt.AutoPrompt.serialize">serialize</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes" href="#imodelsx.iprompt.autoprompt.AutoPrompt.test_prefixes">test_prefixes</a></code></li>
<li><code><a title="imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer" href="#imodelsx.iprompt.autoprompt.AutoPrompt.tokenizer">tokenizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>